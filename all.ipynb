{"cells":[{"cell_type":"markdown","metadata":{"id":"504CGMbmQJFG"},"source":["# DS-GA 1011: More data and training: an improved baseline for Language Models as Unreliable Spatial Reasoners"]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAGQmeNyQW42","executionInfo":{"status":"ok","timestamp":1669501831873,"user_tz":300,"elapsed":371,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"db4c8fcf-1598-42c5-f1a5-1541e9962c77"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Nov 26 22:30:31 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYWPs3CCQZF4","executionInfo":{"status":"ok","timestamp":1669501839326,"user_tz":300,"elapsed":252,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"5aabc007-650d-4b9e-a9ad-17c29ec04603"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}]},{"cell_type":"code","source":["# connect drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"f6_2KETzQgLt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669501861530,"user_tz":300,"elapsed":19208,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"33fa3153-34c8-4c8a-9814-11930d5b06f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/My Drive/babi/\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJvxRDiLRJkS","executionInfo":{"status":"ok","timestamp":1669501863163,"user_tz":300,"elapsed":234,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"956dcaf7-3a0f-4477-cb8c-9407edfc9955"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/babi\n"," all.ipynb                                               README.md\n"," \u001b[0m\u001b[01;34manalysis\u001b[0m/                                               requirements.txt\n"," data.tsv                                                \u001b[01;34mresults\u001b[0m/\n","'Greene setup notes.md'                                  \u001b[01;34mscripts\u001b[0m/\n","'Language Models are Unreliable Spatial Reasoners.pdf'   \u001b[01;34msrc\u001b[0m/\n"," plot.ipynb                                              \u001b[01;34msummary\u001b[0m/\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install openai\n","!pip install sentencepiece"],"metadata":{"id":"2LbR-0UlEMMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669501892654,"user_tz":300,"elapsed":26991,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"8f5e46d7-9fb9-4b46-b6dc-5c7e402657df"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 57.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 90.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.25.0.tar.gz (44 kB)\n","\u001b[K     |████████████████████████████████| 44 kB 1.5 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from openai) (4.1.1)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openai) (1.21.6)\n","Collecting pandas-stubs>=1.1.0.11\n","  Downloading pandas_stubs-1.2.0.62-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 29.5 MB/s \n","\u001b[?25hRequirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n","Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.9.24)\n","Building wheels for collected packages: openai\n","  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=b7a1a884c3ab10b301b2345cbbfd3fbfd8f66201e253257521dbbe1e9a54cf7d\n","  Stored in directory: /root/.cache/pip/wheels/19/de/db/e82770b480ec30fd4a6d67108744b9c52be167c04fcf4af7b5\n","Successfully built openai\n","Installing collected packages: pandas-stubs, openai\n","Successfully installed openai-0.25.0 pandas-stubs-1.2.0.62\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 12.9 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","source":["!python scripts/generate.py"],"metadata":{"id":"ObgUuu2oS0eK","executionInfo":{"status":"ok","timestamp":1669491601596,"user_tz":300,"elapsed":3445,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!python scripts/test.py t5-small\n","!python scripts/test.py t5-large\n","!python scripts/test.py t5-base\n","!python scripts/analyze.py t5-small\n","!python scripts/analyze.py t5-base\n","!python scripts/analyze.py t5-large\n"],"metadata":{"id":"9GQj_7SgS6V3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669492447169,"user_tz":300,"elapsed":840840,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"e178a798-cdd7-4677-d528-41a4da4f3caa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model t5-small...\n","Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.28MB/s]\n","Downloading: 100% 242M/242M [00:04<00:00, 55.9MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 2.27MB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n","Loading model t5-large...\n","Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.19MB/s]\n","Downloading: 100% 2.95G/2.95G [00:40<00:00, 72.8MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 1.83MB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n","Loading model t5-base...\n","Downloading: 100% 1.20k/1.20k [00:00<00:00, 979kB/s]\n","Downloading: 100% 892M/892M [00:30<00:00, 29.7MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 2.29MB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  FutureWarning,\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py microsoft/deberta-base-mnli\n","!python scripts/analyze.py microsoft/deberta-base-mnli\n"],"metadata":{"id":"QsqvoYv9h67V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669492531288,"user_tz":300,"elapsed":84125,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"9cc79787-386c-4b4c-c651-388f64f0f694"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model microsoft/deberta-base-mnli...\n","Downloading: 100% 728/728 [00:00<00:00, 858kB/s]\n","Downloading: 100% 557M/557M [00:13<00:00, 40.2MB/s]\n","Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n","- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 52.0/52.0 [00:00<00:00, 55.4kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 2.09MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py microsoft/deberta-large-mnli\n","!python scripts/analyze.py microsoft/deberta-large-mnli"],"metadata":{"id":"CnrgZ71esF89","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669492766253,"user_tz":300,"elapsed":234971,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"bbc061e6-baa4-4afb-ada5-f63fdd4bdafe"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model microsoft/deberta-large-mnli...\n","Downloading: 100% 729/729 [00:00<00:00, 475kB/s]\n","Downloading: 100% 1.62G/1.62G [00:41<00:00, 39.0MB/s]\n","Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n","- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 52.0/52.0 [00:00<00:00, 53.9kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 1.75MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py allenai/unifiedqa-v2-t5-small-1363200\n","!python scripts/analyze.py allenai/unifiedqa-v2-t5-small-1363200\n"],"metadata":{"id":"iBZAJ_8AvX6E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669493074814,"user_tz":300,"elapsed":308567,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"f86a18b9-8554-4b27-b07e-551626cf951f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model allenai/unifiedqa-v2-t5-small-1363200...\n","Downloading: 100% 1.35k/1.35k [00:00<00:00, 1.53MB/s]\n","Downloading: 100% 242M/242M [00:06<00:00, 37.8MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 1.73MB/s]\n","Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.86MB/s]\n","Downloading: 100% 2.15k/2.15k [00:00<00:00, 2.00MB/s]\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py allenai/unifiedqa-v2-t5-large-1363200\n","!python scripts/analyze.py allenai/unifiedqa-v2-t5-large-1363200"],"metadata":{"id":"a6mV9Pjry62r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669499803415,"user_tz":300,"elapsed":1222567,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"0133818b-032f-4a6f-deef-39d5226e3abd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model allenai/unifiedqa-v2-t5-large-1363200...\n","Downloading: 100% 1.36k/1.36k [00:00<00:00, 1.26MB/s]\n","Downloading: 100% 2.95G/2.95G [02:39<00:00, 18.5MB/s]\n","Downloading: 100% 792k/792k [00:01<00:00, 748kB/s]\n","Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.67MB/s]\n","Downloading: 100% 2.15k/2.15k [00:00<00:00, 1.85MB/s]\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py allenai/unifiedqa-v2-t5-base-1363200\n","!python scripts/analyze.py allenai/unifiedqa-v2-t5-base-1363200"],"metadata":{"id":"NWhPdTHj4rCO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500282068,"user_tz":300,"elapsed":478661,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"52567095-29f4-483d-baeb-36d434fb0c0a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model allenai/unifiedqa-v2-t5-base-1363200...\n","Downloading: 100% 1.36k/1.36k [00:00<00:00, 1.33MB/s]\n","Downloading: 100% 892M/892M [00:50<00:00, 17.7MB/s]\n","Downloading: 100% 792k/792k [00:00<00:00, 57.6MB/s]\n","Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.61MB/s]\n","Downloading: 100% 2.14k/2.14k [00:00<00:00, 1.79MB/s]\n","Model loaded.\n","0\n","/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  UserWarning,\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py roberta-large-mnli\n","!python scripts/analyze.py roberta-large-mnli"],"metadata":{"id":"aNAFppS07Ds3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500488369,"user_tz":300,"elapsed":206309,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"8cca92b4-9874-42ff-b0ee-5878fb73f239"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model roberta-large-mnli...\n","Downloading: 100% 688/688 [00:00<00:00, 561kB/s]\n","Downloading: 100% 1.43G/1.43G [00:22<00:00, 64.0MB/s]\n","Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 899k/899k [00:01<00:00, 808kB/s]\n","Downloading: 100% 456k/456k [00:01<00:00, 414kB/s]\n","Downloading: 100% 1.36M/1.36M [00:01<00:00, 994kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py anirudh21/albert-large-v2-finetuned-mnli\n","!python scripts/analyze.py anirudh21/albert-large-v2-finetuned-mnli"],"metadata":{"id":"oCcQwLhJ_cab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500679086,"user_tz":300,"elapsed":190722,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"fbcf3c55-72da-48ee-83b0-697162e62598"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model anirudh21/albert-large-v2-finetuned-mnli...\n","Downloading: 100% 1.04k/1.04k [00:00<00:00, 998kB/s]\n","Downloading: 100% 70.8M/70.8M [00:04<00:00, 14.2MB/s]\n","Downloading: 100% 468/468 [00:00<00:00, 420kB/s]\n","Downloading: 100% 2.27M/2.27M [00:01<00:00, 1.43MB/s]\n","Downloading: 100% 245/245 [00:00<00:00, 210kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py cross-encoder/nli-deberta-v3-base\n","!python scripts/analyze.py cross-encoder/nli-deberta-v3-base"],"metadata":{"id":"wKezDlwyAtKw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500809440,"user_tz":300,"elapsed":130362,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"775556c6-7a89-4d44-ac4a-4d3bcd0b21c8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model cross-encoder/nli-deberta-v3-base...\n","Downloading: 100% 1.05k/1.05k [00:00<00:00, 749kB/s]\n","Downloading: 100% 738M/738M [00:40<00:00, 18.2MB/s]\n","Downloading: 100% 417/417 [00:00<00:00, 410kB/s]\n","Downloading: 100% 2.46M/2.46M [00:01<00:00, 1.92MB/s]\n","Downloading: 100% 18.0/18.0 [00:00<00:00, 15.7kB/s]\n","Downloading: 100% 156/156 [00:00<00:00, 145kB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py cross-encoder/nli-deberta-v3-small\n","!python scripts/analyze.py cross-encoder/nli-deberta-v3-small"],"metadata":{"id":"EwOHOIs4BcBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500898021,"user_tz":300,"elapsed":88587,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"7d3d7324-8b94-4659-9a7d-961a944782ab"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model cross-encoder/nli-deberta-v3-small...\n","Downloading: 100% 1.05k/1.05k [00:00<00:00, 787kB/s]\n","Downloading: 100% 568M/568M [00:32<00:00, 17.7MB/s]\n","Downloading: 100% 418/418 [00:00<00:00, 390kB/s]\n","Downloading: 100% 2.46M/2.46M [00:01<00:00, 1.94MB/s]\n","Downloading: 100% 18.0/18.0 [00:00<00:00, 16.7kB/s]\n","Downloading: 100% 156/156 [00:00<00:00, 138kB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py cross-encoder/nli-deberta-v3-xsmall\n","!python scripts/analyze.py cross-encoder/nli-deberta-v3-xsmall"],"metadata":{"id":"FcNDO91XDCXI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669500964476,"user_tz":300,"elapsed":66463,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"b6fa6133-0cff-46d8-dbb8-bd688e34a2ff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model cross-encoder/nli-deberta-v3-xsmall...\n","Downloading: 100% 1.05k/1.05k [00:00<00:00, 1.05MB/s]\n","Downloading: 100% 283M/283M [00:16<00:00, 16.7MB/s]\n","Downloading: 100% 419/419 [00:00<00:00, 397kB/s]\n","Downloading: 100% 2.46M/2.46M [00:01<00:00, 1.92MB/s]\n","Downloading: 100% 18.0/18.0 [00:00<00:00, 16.0kB/s]\n","Downloading: 100% 156/156 [00:00<00:00, 138kB/s]\n","/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py cross-encoder/nli-roberta-base\n","!python scripts/analyze.py cross-encoder/nli-roberta-base"],"metadata":{"id":"fablebkmDCmG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669501043833,"user_tz":300,"elapsed":79364,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"ac43cb6f-9d0d-4cdd-bc3a-3624d9a067ad"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model cross-encoder/nli-roberta-base...\n","Downloading: 100% 702/702 [00:00<00:00, 706kB/s]\n","Downloading: 100% 499M/499M [00:07<00:00, 66.5MB/s]\n","Downloading: 100% 25.0/25.0 [00:00<00:00, 22.7kB/s]\n","Downloading: 100% 899k/899k [00:01<00:00, 678kB/s]\n","Downloading: 100% 456k/456k [00:01<00:00, 406kB/s]\n","Downloading: 100% 772/772 [00:00<00:00, 679kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\n","!python scripts/analyze.py ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli"],"metadata":{"id":"5BxkZFAADx7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502187796,"user_tz":300,"elapsed":277058,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"0d1f15bf-5299-47b7-9118-615a786a3647"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli...\n","Downloading: 100% 1.37k/1.37k [00:00<00:00, 1.37MB/s]\n","Downloading: 100% 1.63G/1.63G [00:57<00:00, 28.1MB/s]\n","Downloading: 100% 50.0/50.0 [00:00<00:00, 42.7kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 1.76MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n","Downloading: 100% 772/772 [00:00<00:00, 699kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\n","!python scripts/analyze.py ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli"],"metadata":{"id":"ncrKKLc-Dx-7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669501598953,"user_tz":300,"elapsed":46632,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"6683dd21-a464-4966-ddba-a435cb0c41ce"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli...\n","Downloading: 100% 770/770 [00:00<00:00, 740kB/s]\n","Downloading: 100% 1.34G/1.34G [01:15<00:00, 17.8MB/s]\n","Downloading: 100% 49.0/49.0 [00:00<00:00, 47.8kB/s]\n","Downloading: 100% 232k/232k [00:00<00:00, 258kB/s]\n","Downloading: 100% 112/112 [00:00<00:00, 104kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\n","!python scripts/analyze.py ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli"],"metadata":{"id":"wx0D97rsG419","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502389001,"user_tz":300,"elapsed":201213,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"0f88ea96-9610-4e7e-a619-9ca9159db75b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli...\n","Downloading: 100% 703/703 [00:00<00:00, 728kB/s]\n","Downloading: 100% 1.43G/1.43G [00:20<00:00, 69.6MB/s]\n","Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 49.0/49.0 [00:00<00:00, 48.0kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 1.74MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n","Downloading: 100% 772/772 [00:00<00:00, 754kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\n","!python scripts/analyze.py ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli"],"metadata":{"id":"drIZfehSG5Qw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502608961,"user_tz":300,"elapsed":219966,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"ba08e247-bcec-4c71-b498-e9740568c623"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli...\n","Downloading: 100% 948/948 [00:00<00:00, 880kB/s]\n","Downloading: 100% 1.45G/1.45G [00:34<00:00, 41.3MB/s]\n","Downloading: 100% 24.0/24.0 [00:00<00:00, 23.8kB/s]\n","Downloading: 100% 798k/798k [00:00<00:00, 1.85MB/s]\n","Downloading: 100% 202/202 [00:00<00:00, 179kB/s]\n","Model loaded.\n","0\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py textattack/roberta-base-MNLI\n","!python scripts/analyze.py textattack/roberta-base-MNLI"],"metadata":{"id":"lAflYUg_NF0y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502686763,"user_tz":300,"elapsed":77805,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"ca3a8400-9295-404f-c3bf-a69f55b33291"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model textattack/roberta-base-MNLI...\n","Downloading: 100% 678/678 [00:00<00:00, 647kB/s]\n","Downloading: 100% 501M/501M [00:13<00:00, 37.4MB/s]\n","Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 25.0/25.0 [00:00<00:00, 23.5kB/s]\n","Downloading: 100% 899k/899k [00:00<00:00, 2.06MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n","Downloading: 100% 150/150 [00:00<00:00, 133kB/s]\n","Model loaded.\n","0\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]},{"cell_type":"code","source":["!python scripts/test.py textattack/xlnet-base-cased-MNLI\n","!python scripts/analyze.py textattack/xlnet-base-cased-MNLI"],"metadata":{"id":"limysm76NF3k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669502762547,"user_tz":300,"elapsed":75791,"user":{"displayName":"Wei Qiuyi","userId":"10983350560931001358"}},"outputId":"7aed561f-db81-44cc-be5d-1bf6fdd2dc00"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model textattack/xlnet-base-cased-MNLI...\n","Downloading: 100% 958/958 [00:00<00:00, 931kB/s]\n","Downloading: 100% 469M/469M [00:11<00:00, 39.5MB/s]\n","Downloading: 100% 2.00/2.00 [00:00<00:00, 1.95kB/s]\n","Downloading: 100% 798k/798k [00:00<00:00, 1.84MB/s]\n","Downloading: 100% 202/202 [00:00<00:00, 172kB/s]\n","Model loaded.\n","0\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n","270\n","280\n","290\n","300\n","310\n","320\n","330\n","340\n","350\n","360\n","370\n","380\n","390\n","400\n","410\n","420\n","430\n","440\n","450\n","460\n","470\n","480\n","490\n","500\n","510\n","520\n","530\n","540\n","550\n","560\n","570\n","580\n","590\n","600\n","610\n","620\n","630\n","640\n","650\n","660\n","670\n","680\n","690\n","700\n","710\n","720\n","730\n","740\n","750\n","760\n","770\n","780\n","790\n","800\n","810\n","820\n","830\n","840\n","850\n","860\n","870\n","880\n","890\n","900\n","910\n","920\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}